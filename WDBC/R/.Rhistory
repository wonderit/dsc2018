mod_knn = knnreg(f, data=tmp.train, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
}
tmp.train
xtrain[[1]]
nM = 10;
rmse = data.frame(Origin=rep(0, nM),
Norm=rep(0, nM),
MinMax=rep(0, nM))
rownames(rmse) = c("knn", "linear regression", "multi linear regression", "lasso",
"decision tree", "random forest", "boosted tree", "svm",
"single layer ann", "xgboost")
R2 = rmse
rownames(rmse)=rownames(rmse)
#data = read.csv("../data/speeddating.csv");
train = read.csv("../data/speeddating_likeo_train_yj.csv");
train = train[,-(1:2)];
train = train[,!(colnames(train)=="decision_o")]
test = read.csv("../data/speeddating_likeo_test_yj.csv");
test = test[,-(1:2)];
test = test[,!(colnames(test)=="decision_o")]
train.col = colnames(train)
yvar = "like_o"
remove.var = c(yvar);
use.xvar = setdiff(train.col, remove.var)
# Original data
xtrain =as.matrix(train[,use.xvar])
ytrain = train[,yvar];
xtest = as.matrix(test[,use.xvar]);
ytest = test[,yvar];
# Normalized data
xtrain.N = scale(xtrain);
xtest.N = scale(xtest);
# MinMax scale
MinMax = function(x){ return((x-min(x))/(max(x)-min(x))); }
xtrain.MM = apply(xtrain, 2, MinMax);
xtest.MM = apply(xtest, 2, MinMax);
xtrain = list(O=xtrain, N=xtrain.N, MM=xtrain.MM);
xtest = list(O=xtest, N=xtest.N, mm=xtest.MM);
for(i in 1:ncol(rmse)){
# full model formula
n = c(names(train));
f = as.formula(paste("like_o ~", paste(n[!n %in% "like_o"], collapse = "+")));
tmp.train = data.frame(xtrain[[i]], ytrain);
colnames(tmp.train) = c(colnames(xtrain[[i]], colnames(ytrain)))
# knn regression
mod_knn = knnreg(f, data=tmp.train, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
}
tmp.train
for(i in 1:ncol(rmse)){
# full model formula
n = c(names(train));
f = as.formula(paste("like_o ~", paste(n[!n %in% "like_o"], collapse = "+")));
tmp.train = data.frame(xtrain[[i]], "like_o"=ytrain);
# knn regression
mod_knn = knnreg(f, data=tmp.train, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
}
tmp.train
is.data.frame(tmp.train)
# knn regression
mod_knn = knnreg(f, data=tmp.train, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
for(i in 1:ncol(rmse)){
# full model formula
n = c(names(train));
f = as.formula(paste("like_o ~", paste(n[!n %in% "like_o"], collapse = "+")));
tmp.train = data.frame(xtrain[[i]], "like_o"=ytrain);
# knn regression
mod_knn = knnreg(x=xtrain[[i]], y=ytrain, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
}
rmse
for(i in 1:ncol(rmse)){
# full model formula
n = c(names(train));
f = as.formula(paste("like_o ~", paste(n[!n %in% "like_o"], collapse = "+")));
tmp.train = data.frame(xtrain[[i]], "like_o"=ytrain);
# knn regression
mod_knn = knnreg(x=xtrain[[i]], y=ytrain, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
# linear regression via each variable
score_lm = rep(ncol(xtrain[[i]]));
for(j in 1:ncol(xtrain)){
tmp.data = data.frame(x=xtrain[[i]][,j], y=ytrain);
mod_lm = lm(y~x, data=tmp.data)
tmp.test = data.frame(x=xtest[[i]][,j]);
tmp.pred_lm = predict(mod_lm, tmp.test);
score_lm[j]= sqrt(mean((ytest-tmp.pred_lm)^2))
}
(rmse["linear regression", i] = min(score_lm));
print("Linear regression done")
# linear regression via all variables
mod_alm = lm(f, data=tmp.train)
pred_alm = predict(mod_alm, as.data.frame(xtest[[i]]))
(rmse["multi linear regression", i] = sqrt(mean((ytest-pred_alm)^2)))
# lasso regression
cv.mod_lasso = cv.glmnet(x = xtrain[[i]], y=ytrain, alpha=1)
lambda = cv.mod_lasso$lambda.1se;
mod_lasso = glmnet(x = xtrain[[i]], y=ytrain, lambda = lambda);
pred_lasso = predict(mod_lasso, xtest[[i]])
(rmse["lasso", i] = sqrt(mean((ytest-pred_lasso)^2)));
print("Lasso done");
}
for(i in 1:ncol(rmse)){
# full model formula
n = c(names(train));
f = as.formula(paste("like_o ~", paste(n[!n %in% "like_o"], collapse = "+")));
tmp.train = data.frame(xtrain[[i]], "like_o"=ytrain);
# knn regression
mod_knn = knnreg(x=xtrain[[i]], y=ytrain, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
# linear regression via each variable
score_lm = rep(ncol(xtrain[[i]]));
for(j in 1:ncol(xtrain[[i]])){
tmp.data = data.frame(x=xtrain[[i]][,j], y=ytrain);
mod_lm = lm(y~x, data=tmp.data)
tmp.test = data.frame(x=xtest[[i]][,j]);
tmp.pred_lm = predict(mod_lm, tmp.test);
score_lm[j]= sqrt(mean((ytest-tmp.pred_lm)^2))
}
(rmse["linear regression", i] = min(score_lm));
print("Linear regression done")
# linear regression via all variables
mod_alm = lm(f, data=tmp.train)
pred_alm = predict(mod_alm, as.data.frame(xtest[[i]]))
(rmse["multi linear regression", i] = sqrt(mean((ytest-pred_alm)^2)))
# lasso regression
cv.mod_lasso = cv.glmnet(x = xtrain[[i]], y=ytrain, alpha=1)
lambda = cv.mod_lasso$lambda.1se;
mod_lasso = glmnet(x = xtrain[[i]], y=ytrain, lambda = lambda);
pred_lasso = predict(mod_lasso, xtest[[i]])
(rmse["lasso", i] = sqrt(mean((ytest-pred_lasso)^2)));
print("Lasso done");
}
rmse
for(i in 1:ncol(rmse)){
# full model formula
n = c(names(train));
f = as.formula(paste("like_o ~", paste(n[!n %in% "like_o"], collapse = "+")));
tmp.train = data.frame(xtrain[[i]], "like_o"=ytrain);
# knn regression
mod_knn = knnreg(x=xtrain[[i]], y=ytrain, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
# linear regression via each variable
score_lm = rep(ncol(xtrain[[i]]));
for(j in 1:ncol(xtrain[[i]])){
tmp.data = data.frame(x=xtrain[[i]][,j], y=ytrain);
mod_lm = lm(y~x, data=tmp.data)
tmp.test = data.frame(x=xtest[[i]][,j]);
tmp.pred_lm = predict(mod_lm, tmp.test);
score_lm[j]= sqrt(mean((ytest-tmp.pred_lm)^2))
}
(rmse["linear regression", i] = min(score_lm));
print("Linear regression done")
# linear regression via all variables
mod_alm = lm(f, data=tmp.train)
pred_alm = predict(mod_alm, as.data.frame(xtest[[i]]))
(rmse["multi linear regression", i] = sqrt(mean((ytest-pred_alm)^2)))
# lasso regression
cv.mod_lasso = cv.glmnet(x = xtrain[[i]], y=ytrain, alpha=1)
lambda = cv.mod_lasso$lambda.1se;
mod_lasso = glmnet(x = xtrain[[i]], y=ytrain, lambda = lambda);
pred_lasso = predict(mod_lasso, xtest[[i]])
(rmse["lasso", i] = sqrt(mean((ytest-pred_lasso)^2)));
print("Lasso done");
# random forest
mod_randomf = randomForest(x = xtrain[[i]], y = ytrain)
pred_randomf = predict(mod_randomf, xtest[[i]])
(rmse["random forest", i] = sqrt(mean((ytest-pred_randomf)^2)))
}
# xgboost
mod_xgb = xgboost(data = xtrain[[1]], label = ytrain, nround=10)
pred_xgb = predict(mod_xgb, xtest[[1]])
(xgb.acc = sqrt(mean((ytest-pred_xgb)^2)))
pred_xgb
mod_xgb = xgboost(data = xtrain[[1]], label = ytrain, nround=300)
pred_xgb = predict(mod_xgb, xtest[[1]])
(xgb.acc = sqrt(mean((ytest-pred_xgb)^2)))
help("xgboost")
# xgboost
mod_xgb = xgb.train(data = xtrain[[1]], label = ytrain, nround=300)
pred_xgb = predict(mod_xgb, xtest[[1]])
(xgb.acc = sqrt(mean((ytest-pred_xgb)^2)))
# xgboost
xgb.param = list(max_depth=2)
mod_xgb = xgb.train(params = xgb.param, data = xtrain[[1]], label = ytrain, nround=300, verbose = FALSE)
pred_xgb = predict(mod_xgb, xtest[[1]])
(xgb.acc = sqrt(mean((ytest-pred_xgb)^2)))
xgb.param = list(max_depth=2)
xgb.data = xgb.DMatrix(train[[1]], label=ytrain)
mod_xgb = xgb.train(params = xgb.param, data = xgb.data, nround=300, verbose = FALSE)
pred_xgb = predict(mod_xgb, xtest[[1]])
(xgb.acc = sqrt(mean((ytest-pred_xgb)^2)))
xgb.dtrain = xgb.DMatrix(train[[1]], label=ytrain)
# xgboost
mod_xgb = xgb.train(data = xtrain[[1]], label = ytrain, nround=300, verbose = FALSE)
# xgboost
mod_xgb = xgboost(data = xtrain[[1]], label = ytrain, nround=300, verbose = FALSE)
# xgboost
mod_xgb = xgboost(data = xtrain[[1]], label = ytrain, nround=300, verbose = FALSE,
objective="reg:linear", eval_metric="rmse", eta=0.01, gamma=0.05, max_depth=6)
pred_xgb = predict(mod_xgb, xtest[[1]])
(xgb.acc = sqrt(mean((ytest-pred_xgb)^2)))
# xgboost
mod_xgb = xgboost(data = xtrain[[1]], label = ytrain, nround=1000, verbose = FALSE,
objective="reg:linear", eval_metric="rmse", eta=0.01, gamma=0.05, max_depth=6)
pred_xgb = predict(mod_xgb, xtest[[1]])
(xgb.acc = sqrt(mean((ytest-pred_xgb)^2)))
# xgboost
mod_xgb = xgboost(data = xtrain[[1]], label = ytrain, nround=1000, verbose = FALSE,
objective="reg:linear", eval_metric="rmse", eta=0.01, gamma=0.05, max_depth=6,
min_child_weight = 1.7817, subsample = 0.5213, colsample_bytree = 0.4603)
pred_xgb = predict(mod_xgb, xtest[[1]])
(xgb.acc = sqrt(mean((ytest-pred_xgb)^2)))
for(i in 1:ncol(rmse)){
# full model formula
n = c(names(train));
f = as.formula(paste("like_o ~", paste(n[!n %in% "like_o"], collapse = "+")));
tmp.train = data.frame(xtrain[[i]], "like_o"=ytrain);
# knn regression
mod_knn = knnreg(x=xtrain[[i]], y=ytrain, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
# linear regression via each variable
score_lm = rep(ncol(xtrain[[i]]));
for(j in 1:ncol(xtrain[[i]])){
tmp.data = data.frame(x=xtrain[[i]][,j], y=ytrain);
mod_lm = lm(y~x, data=tmp.data)
tmp.test = data.frame(x=xtest[[i]][,j]);
tmp.pred_lm = predict(mod_lm, tmp.test);
score_lm[j]= sqrt(mean((ytest-tmp.pred_lm)^2))
}
(rmse["linear regression", i] = min(score_lm));
print("Linear regression done")
# linear regression via all variables
mod_alm = lm(f, data=tmp.train)
pred_alm = predict(mod_alm, as.data.frame(xtest[[i]]))
(rmse["multi linear regression", i] = sqrt(mean((ytest-pred_alm)^2)))
# lasso regression
cv.mod_lasso = cv.glmnet(x = xtrain[[i]], y=ytrain, alpha=1)
lambda = cv.mod_lasso$lambda.1se;
mod_lasso = glmnet(x = xtrain[[i]], y=ytrain, lambda = lambda);
pred_lasso = predict(mod_lasso, xtest[[i]])
(rmse["lasso", i] = sqrt(mean((ytest-pred_lasso)^2)));
print("Lasso done");
# decision tree
# random forest
mod_randomf = randomForest(x = xtrain[[i]], y = ytrain)
pred_randomf = predict(mod_randomf, xtest[[i]])
(rmse["random forest", i] = sqrt(mean((ytest-pred_randomf)^2)))
print("Random forest done");
# SVM
mod_svm = svm(f, data=tmp.train)
#mod_svm = tune(svm, train.x=xtrain, train.y=ytrain, kernel="radial", ranges = list(cost=10^(-1:2), gamma=c(.5,1,2)))
pred.svm = predict(mod_svm, xtest[[i]])
(rmse["svm", i] = sqrt(mean((test[,"like_o"]-pred.svm)^2)))
print("SVM done");
# xgboost
mod_xgb = xgboost(data = xtrain[[i]], label = ytrain, nround=1000, verbose = FALSE,
objective="reg:linear", eval_metric="rmse", eta=0.01, gamma=0.05, max_depth=6,
min_child_weight = 1.7817, subsample = 0.5213, colsample_bytree = 0.4603)
pred_xgb = predict(mod_xgb, xtest[[i]])
(rmse["xgboost", i] = sqrt(mean((ytest-pred_xgb)^2)))
print("Xgboost done");
}
rmse
library(iterators)
library(parallel)
install.packages("doMC")
library(doMC)
cl = makeCluster(4)
registerDoParallel(cl);
caret.train.ctrl = trainControl(method="repeatedcv", number=5, repeat=5, verboseIter=FALSE, allowParallel=TRUE);
mod_gbm = train(f, method="gbm", metric="RMSE", maximize=FALSE, trControl=caret.train.ctrl,
tuneGrid=expand.grid(n.trees=(4:10)*50, interaction.depth=c(5), shrinkage=c(0.05),
n.minobsinnode=c(10)), data=tmp.train, verbose=FALSE);
cl = makeCluster(4)
registerDoParallel(cl);
caret.train.ctrl = trainControl(method="repeatedcv", number=5, repeat=5, verboseIter=FALSE, allowParallel=TRUE);
caret.train.ctrl <- trainControl(method="repeatedcv", number=5, repeat=5, verboseIter=FALSE, allowParallel=TRUE);
caret.train.ctrl = trainControl(method="repeatedcv", number=5, repeats=5, verboseIter=FALSE, allowParallel=TRUE);
mod_gbm = train(f, method="gbm", metric="RMSE", maximize=FALSE, trControl=caret.train.ctrl,
tuneGrid=expand.grid(n.trees=(4:10)*50, interaction.depth=c(5), shrinkage=c(0.05),
n.minobsinnode=c(10)), data=tmp.train, verbose=FALSE);
pred_gbm = predict(mod_gbm, xtest[[i]])
pred_gbm
sqrt(mean((ytest-pred_gbm)^2))
caret.train.ctrl = trainControl(method="repeatedcv", number=5, repeats=5, verboseIter=FALSE, allowParallel=FALSE);
mod_gbm = train(f, method="gbm", metric="RMSE", maximize=FALSE, trControl=caret.train.ctrl,
tuneGrid=expand.grid(n.trees=(4:10)*50, interaction.depth=c(5), shrinkage=c(0.05),
n.minobsinnode=c(10)), data=tmp.train, verbose=FALSE);
pred_gbm = predict(mod_gbm, xtest[[i]])
sqrt(mean((ytest-pred_gbm)^2))
for(i in 1:ncol(rmse)){
# full model formula
n = c(names(train));
f = as.formula(paste("like_o ~", paste(n[!n %in% "like_o"], collapse = "+")));
tmp.train = data.frame(xtrain[[i]], "like_o"=ytrain);
# knn regression
mod_knn = knnreg(x=xtrain[[i]], y=ytrain, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
# linear regression via each variable
score_lm = rep(ncol(xtrain[[i]]));
for(j in 1:ncol(xtrain[[i]])){
tmp.data = data.frame(x=xtrain[[i]][,j], y=ytrain);
mod_lm = lm(y~x, data=tmp.data)
tmp.test = data.frame(x=xtest[[i]][,j]);
tmp.pred_lm = predict(mod_lm, tmp.test);
score_lm[j]= sqrt(mean((ytest-tmp.pred_lm)^2))
}
(rmse["linear regression", i] = min(score_lm));
print("Linear regression done")
# linear regression via all variables
mod_alm = lm(f, data=tmp.train)
pred_alm = predict(mod_alm, as.data.frame(xtest[[i]]))
(rmse["multi linear regression", i] = sqrt(mean((ytest-pred_alm)^2)))
# lasso regression
cv.mod_lasso = cv.glmnet(x = xtrain[[i]], y=ytrain, alpha=1)
lambda = cv.mod_lasso$lambda.1se;
mod_lasso = glmnet(x = xtrain[[i]], y=ytrain, lambda = lambda);
pred_lasso = predict(mod_lasso, xtest[[i]])
(rmse["lasso", i] = sqrt(mean((ytest-pred_lasso)^2)));
print("Lasso done");
# decision tree
# random forest
mod_randomf = randomForest(x = xtrain[[i]], y = ytrain)
pred_randomf = predict(mod_randomf, xtest[[i]])
(rmse["random forest", i] = sqrt(mean((ytest-pred_randomf)^2)))
print("Random forest done");
# SVM
mod_svm = svm(f, data=tmp.train)
#mod_svm = tune(svm, train.x=xtrain, train.y=ytrain, kernel="radial", ranges = list(cost=10^(-1:2), gamma=c(.5,1,2)))
pred.svm = predict(mod_svm, xtest[[i]])
(rmse["svm", i] = sqrt(mean((test[,"like_o"]-pred.svm)^2)))
print("SVM done");
# gradiant boost
caret.train.ctrl = trainControl(method="repeatedcv", number=5, repeats=5, verboseIter=FALSE, allowParallel=FALSE);
mod_gbm = train(f, method="gbm", metric="RMSE", maximize=FALSE, trControl=caret.train.ctrl,
tuneGrid=expand.grid(n.trees=(4:10)*50, interaction.depth=c(5), shrinkage=c(0.05),
n.minobsinnode=c(10)), data=tmp.train, verbose=FALSE);
pred_gbm = predict(mod_gbm, xtest[[i]])
rmse["gradiant boost", i] = sqrt(mean((ytest-pred_gbm)^2))
# xgboost
mod_xgb = xgboost(data = xtrain[[i]], label = ytrain, nround=1000, verbose = FALSE,
objective="reg:linear", eval_metric="rmse", eta=0.01, gamma=0.05, max_depth=6,
min_child_weight = 1.7817, subsample = 0.5213, colsample_bytree = 0.4603)
pred_xgb = predict(mod_xgb, xtest[[i]])
(rmse["xgboost", i] = sqrt(mean((ytest-pred_xgb)^2)))
print("Xgboost done");
}
rmse
mod_dt = rpart(f, method="anova", data=tmp.train)
library(rpart)
mod_dt = rpart(f, method="anova", data=tmp.train)
pred_dt = predict(mod_dt, xtest[[3]])
pred_dt = predict(mod_dt, as.data.frame(xtest[[3]]))
sqrt(mean((ytest-pred_dt)^2))
mod_dt = ctree(f, data=tmp.train, controls=ctree_control(minsplit=2,minbucket=2,testtype="Univariate"))
mod_dt = ctree(f, data=tmp.train, controls=ctree_control(minsplit=2,minbucket=2,testtype="Multivariate"))
mod_dt = ctree(f, data=tmp.train, controls=ctree_control(minsplit=2,minbucket=2,testtype="MonteCarlo"))
help("ctree_control")
mod_dt = ctree(f, data=tmp.train, controls=ctree_control(testtype="MonteCarlo"))
mod_dt = ctree(f, data=tmp.train)
pred_dt = predict(mod_dt, as.data.frame(xtest[[3]]))
sqrt(mean((ytest-pred_dt)^2))
mod_dt = M5P(f, data=tmp.train)
install.packages("RWeKa")
install.packages("RWeka")
library(RWeka)
mod_dt = M5P(f, data=tmp.train)
pred_dt = predict(mod_dt, as.data.frame(xtest[[3]]))
sqrt(mean((ytest-pred_dt)^2))
install.packages("ipred")
install.packages("ipred")
library(ipred)
mod_dt = Bagging(f, data=tmp.train)
pred_dt = predict(mod_dt, as.data.frame(xtest[[3]]))
sqrt(mean((ytest-pred_dt)^2))
mod_dt = Bagging(f, data=tmp.train, control=rpart.control(minsplit=5))
rmse
nM = 10;
rmse = data.frame(Origin=rep(0, nM),
Norm=rep(0, nM),
MinMax=rep(0, nM))
rownames(rmse) = c("knn", "linear regression", "multi linear regression", "lasso",
"bagging cart", "random forest", "gradient boost", "svm",
"single layer ann", "xgboost")
R2 = rmse
rownames(rmse)=rownames(rmse)
#data = read.csv("../data/speeddating.csv");
train = read.csv("../data/speeddating_likeo_train_yj.csv");
train = train[,-(1:2)];
train = train[,!(colnames(train)=="decision_o")]
test = read.csv("../data/speeddating_likeo_test_yj.csv");
test = test[,-(1:2)];
test = test[,!(colnames(test)=="decision_o")]
train.col = colnames(train)
yvar = "like_o"
remove.var = c(yvar);
use.xvar = setdiff(train.col, remove.var)
# Original data
xtrain =as.matrix(train[,use.xvar])
ytrain = train[,yvar];
xtest = as.matrix(test[,use.xvar]);
ytest = test[,yvar];
# Normalized data
xtrain.N = scale(xtrain);
xtest.N = scale(xtest);
# MinMax scale
MinMax = function(x){ return((x-min(x))/(max(x)-min(x))); }
xtrain.MM = apply(xtrain, 2, MinMax);
xtest.MM = apply(xtest, 2, MinMax);
xtrain = list(O=xtrain, N=xtrain.N, MM=xtrain.MM);
xtest = list(O=xtest, N=xtest.N, mm=xtest.MM);
for(i in 1:ncol(rmse)){
# full model formula
n = c(names(train));
f = as.formula(paste("like_o ~", paste(n[!n %in% "like_o"], collapse = "+")));
tmp.train = data.frame(xtrain[[i]], "like_o"=ytrain);
# knn regression
mod_knn = knnreg(x=xtrain[[i]], y=ytrain, k=20)
pred_knn = predict(mod_knn, xtest[[i]]);
(rmse["knn", i] = sqrt(mean((ytest-pred_knn)^2)));
print("KNN done");
# linear regression via each variable
score_lm = rep(ncol(xtrain[[i]]));
for(j in 1:ncol(xtrain[[i]])){
tmp.data = data.frame(x=xtrain[[i]][,j], y=ytrain);
mod_lm = lm(y~x, data=tmp.data)
tmp.test = data.frame(x=xtest[[i]][,j]);
tmp.pred_lm = predict(mod_lm, tmp.test);
score_lm[j]= sqrt(mean((ytest-tmp.pred_lm)^2))
}
(rmse["linear regression", i] = min(score_lm));
print("Linear regression done")
# linear regression via all variables
mod_alm = lm(f, data=tmp.train)
pred_alm = predict(mod_alm, as.data.frame(xtest[[i]]))
(rmse["multi linear regression", i] = sqrt(mean((ytest-pred_alm)^2)))
# lasso regression
cv.mod_lasso = cv.glmnet(x = xtrain[[i]], y=ytrain, alpha=1)
lambda = cv.mod_lasso$lambda.1se;
mod_lasso = glmnet(x = xtrain[[i]], y=ytrain, lambda = lambda);
pred_lasso = predict(mod_lasso, xtest[[i]])
(rmse["lasso", i] = sqrt(mean((ytest-pred_lasso)^2)));
print("Lasso done");
# decision tree
mod_dt = Bagging(f, data=tmp.train)
pred_dt = predict(mod_dt, as.data.frame(xtest[[3]]))
rmse["bagging cart", i] = sqrt(mean((ytest-pred_dt)^2))
# random forest
mod_randomf = randomForest(x = xtrain[[i]], y = ytrain)
pred_randomf = predict(mod_randomf, xtest[[i]])
(rmse["random forest", i] = sqrt(mean((ytest-pred_randomf)^2)))
print("Random forest done");
# gradient boost
caret.train.ctrl = trainControl(method="repeatedcv", number=5, repeats=5, verboseIter=FALSE, allowParallel=FALSE);
mod_gbm = train(f, method="gbm", metric="RMSE", maximize=FALSE, trControl=caret.train.ctrl,
tuneGrid=expand.grid(n.trees=(4:10)*50, interaction.depth=c(5), shrinkage=c(0.05),
n.minobsinnode=c(10)), data=tmp.train, verbose=FALSE);
pred_gbm = predict(mod_gbm, xtest[[i]])
rmse["gradiant boost", i] = sqrt(mean((ytest-pred_gbm)^2))
print("Gradient boost done")
# SVM
mod_svm = svm(f, data=tmp.train)
#mod_svm = tune(svm, train.x=xtrain, train.y=ytrain, kernel="radial", ranges = list(cost=10^(-1:2), gamma=c(.5,1,2)))
pred.svm = predict(mod_svm, xtest[[i]])
(rmse["svm", i] = sqrt(mean((test[,"like_o"]-pred.svm)^2)))
print("SVM done");
# single layer ANN
nn = ann(x=xtrain[[i]], y=ytrain, size=5, act_hid="tanh", act_out = "linear")
pred.nn = predict(nn, xtest[[i]]);
(rmse["single layer ann", i] = sqrt(mean((ytest-pred.nn)^2)))
# xgboost
mod_xgb = xgboost(data = xtrain[[i]], label = ytrain, nround=1000, verbose = FALSE,
objective="reg:linear", eval_metric="rmse", eta=0.01, gamma=0.05, max_depth=6,
min_child_weight = 1.7817, subsample = 0.5213, colsample_bytree = 0.4603)
pred_xgb = predict(mod_xgb, xtest[[i]])
(rmse["xgboost", i] = sqrt(mean((ytest-pred_xgb)^2)))
print("Xgboost done");
}
rmse
pred_ann
pred_nn
pred.nn
R_sq = function(y, pred){
ym = mean(y);
sstot = sum((y-ym)^2);
ssres = sum((y-pred)^2);
r2 = 1 - (ssres/sstot);
}
rmse
help("ann")
